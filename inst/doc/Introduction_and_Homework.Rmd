---
title: "Introduction to R-package named SA24204187"
author: "Jianjun Zhang"
date: "2024-12-01"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to R-package named SA24204187}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction to "SA24204187" R-package

_SA24204187_ is a simple R package developed to compute kinds of mean estimators of $n$ column vectors of a design matrix over $\mathbb{R}^{p*n}$, $p$ is the dimension of random vectors and $n$ is the number of random vectors. there are simple mean estimators like _empirical.mean_, _CG.mean_, _selectCG.mean_, _trimmed.mean_, _robust.mean_ and _MOM.mean_. _empirical.mean_ is a classical mean estimator with formula $\bar{\mu}_n = \frac{1}{n}\sum^{n}_{i=1} X_i$. In addition, there is a iterative algorithm via weighted mean method to estimate the unknowed sample mean that is robust and can solve the issue that the data has outliers. Simple mean estimators were introducted in the document: Lugosi, G., Mendelson, S. Mean Estimation and Regression Under Heavy-Tailed Distributions: A Survey. Found Comput Math 19, 1145–1190 (2019). The iterative algorithm was introducted in the document: Cheng, Yu et al. "High-Dimensional Robust Mean Estimation via Gradient Descent." International Conference on Machine Learning (2020).

_CG.mean_ and _selectCG.mean_ are the functions with the Catoni–Giulini mean estimator. _selectCG.mean_ is the Catoni–Giulini mean estimator selected by MSE using R. $$\widehat{\mu}_n=\frac{1}{n} \sum_{i=1}^n X_i \min \left(1, \frac{1}{\alpha\left\|X_i\right\|}\right),$$ where $\alpha > 0$ is a (small) parameter.

```{r}
rm(list=ls())
library(SA24204187)
set.seed(123)
random_matrix_normal <- matrix(rnorm(20000, mean = 0, sd = 1), nrow = 200, ncol = 100)
CG.mean(random_matrix_normal)
selectCG.mean(random_matrix_normal,0.05,20)
```

_trimmed.mean_  is defined by removing a fraction of the sample, consisting of the $\epsilon$ largest and smallest points for some parameter $\epsilon \in (0,1)$ and thenaveraging over the rest.

```{r}
rm(list=ls())
library(SA24204187)
set.seed(123)
random_matrix_normal <- matrix(rnorm(20000, mean = 0, sd = 1), nrow = 200, ncol = 100)
trimmed.mean(random_matrix_normal)
```

_robust.mean_ is the function of mean estimation that users can choose variate robust loss functions to solve the mean estimator. There are kinds of loss functions, such as Huber loss, bisquare loss and Wang's loss.

```{r}
rm(list=ls())
library(SA24204187)
set.seed(123)
random_matrix_normal <- matrix(rnorm(20000, mean = 0, sd = 1), nrow = 200, ncol = 100)
robust.mean(random_matrix_normal,method = "huber") 
```

_MOM.mean_ is the function of mean estimation that users can choose some multivariate medians to solve the mean estimator, such as coordinate-wise median, geometric (or spatial) median and Euclidean ball median. Median-of-mean estimator is a mean estimation method: Regardless of what notion of a multivariate median we decide to adopt, we start by partitioning $[n]=\{1, \ldots, n\}$ into $k$ blocks $B_1, \ldots, B_k$, each of size $\left|B_i\right| \geq\lfloor n / k\rfloor \geq 2$. Here $k$ is a parameter of the estimator to be chosen later. For simplicity, we assume that $k m=n$ for some positive integer $m$. Just like before, we compute the sample mean of the random vectors within each block: For $j=1, \ldots, k$, let $$Z_j=\frac{1}{m} \sum_{i \in B_j} X_i,$$ then the median of $Z_j$ is the result of the method.

```{r}
rm(list=ls())
library(SA24204187)
set.seed(123)
random_matrix_normal <- matrix(rnorm(20000, mean = 0, sd = 1), nrow = 200, ncol = 100)
MOM.mean(random_matrix_normal,k = 40,method = "geometric") 
```

To summarize the above, the empirical mean estimate is calculated quickly, but not robustly; The rest of the estimators have a certain degree of robustness, but some of them have poor estimation effect and some of them are computationally complex. _projected_gradient_descent_ is a function via weighted mean estimation that used by gradient descent algorithm. This function not only ensures the robustness and accuracy of the estimation, but also achieves good computational complexity. For more information, please refer to the previously mentioned paper.

```{r}
rm(list=ls())
library(SA24204187)
set.seed(123)
random_matrix_normal <- matrix(rnorm(20000, mean = 0, sd = 1), nrow = 200, ncol = 100)
outliers = matrix(rnorm(2000, mean = 5, sd = 10), nrow = 200, ncol = 10)
X <- cbind(random_matrix_normal,outliers)
epsilon <- 0.1
# 调用投影梯度下降算法
w <- projected_gradient_descent(X, epsilon)
# 计算加权均值
mu_w <- X %*% w
# 输出结果
print("加权均值:")
print(mu_w)
```

# Homework-2024-09-09

## Question

Use knitr to produce at least 3 examples. For each example,
texts should mix with figures and/or tables. Better to have
mathematical formulas.

## Answer

### Analysis of the question

* Find out sufficient data from books, Internet and R positively.

* Using texts, tables, and figures to indicate kinds of characteristics of these data obviously.

* Using mathematical formulas in the text as much as possible.

### Examples

* Example 1: analyse the data named iris in R.

Using `head()` to obverse the data :

```{r}
attach(iris)
head(iris)
```

Now, let's do some little experiments:

```{r}
table(iris$Species) #Check the number of flowers in different species
library(xtable)
xtable::xtable(table(iris$Species))
detach(iris)
```

\begin{table}[ht]
\caption{the number of flowers in different species}
\centering
\begin{tabular}{rr}
  \hline
 & V1 \\ 
  \hline
setosa &  50 \\ 
  versicolor &  50 \\ 
  virginica &  50 \\ 
   \hline
\end{tabular}
\end{table}

* Example 2: analyse the data named freeny.x in R.

The data are notes of four factors that affect revenue. Now, let's do some little experiments:

```{r}
plot(freeny.x[,1],freeny.x[,2],xlab = "lag quarterly revenue",ylab = "price index")
LM = lm(freeny.x[,1]~freeny.x[,2])
summary(LM)
```
The $R^2$ is `r summary(LM)$r.squared`.

* Example 3: analyse the data named swiss in R.

Firstly, using `head()` to obverse the data :

```{r}
head(swiss)
```

Using `plot()` to get scatter plots:

```{r}
plot(swiss)

```

# Homework-2024-09-14

## Question 1 (Exercise 3.4 in "Statistical Computing with R")

The Rayleigh density is $$f(x)=\frac{x}{\sigma^2} e^{-x^2 /\left(2 \sigma^2\right)}, \quad x \geq 0, \sigma>0.$$ Develop an algorithm to generate random samples from a Rayleigh($\sigma$) distribution. Generate Rayleigh($\sigma$) samples for several choices of $\sigma > 0$ and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram).

## Answer

### Analysis

Because of the continuous pdf of the distribution, we can use the method named " the inverse transform method" to generate random samples from some Rayleigh($\sigma$) distributions whose parameters $\sigma$ are  different and graph the relative histograms. Note that the cdf of the distribution is $$F(x) = 1 - exp(-x^2/(2*\sigma^2)) ,\quad x \geq 0.$$ We can check the histograms in order to check that the mode of the generated samples is close to the theoretical mode.

### Examples

We assume that the number of random samples is $1000$, the parameters $\sigma$ are 1, 2 and 3. Just do what the content of the analysis needs to do.

```{r}
rm(list=ls())
n <- 1000
u <- runif(n) # Uniform  distribution
sigma <- 1 # sigma > 0
x <- sqrt(-2*sigma^2*log(1-u)) # F(x) = 1 - exp(-x^2/(2*sigma^2)) , x >= 0
hist(x, prob = TRUE, main = expression(f(x) == (x / sigma^2)  * exp( - x^{2} / (2 * sigma ^ 2)) , sigma = 1),ylim = c(0,0.6))
y <- seq(0, 5, .01)
lines(y, (y / sigma^2) 
      * exp( - y^{2} / (2 * sigma ^ 2)))
```

The plot above is the histogram under the parameter $\sigma$ is 1. Similarly, the plots below are the conditions on the parameters $\sigma$ are 2 and 3.

```{r,echo=FALSE}
rm(list=ls())
par(mfrow=c(1,2))
n <- 1000
u <- runif(n) # Uniform  distribution
sigma <- 2 # sigma > 0
x <- sqrt(-2*sigma^2*log(1-u)) # F(x) = 1 - exp(-x^2/(2*sigma^2)) , x >= 0
hist(x, prob = TRUE, main = expression(sigma == 2),ylim = c(0,0.4))
y <- seq(0, 8, .01)
lines(y, (y / sigma^2) 
      * exp( - y^{2} / (2 * sigma ^ 2)))

sigma <- 3 # sigma > 0
x <- sqrt(-2*sigma^2*log(1-u)) # F(x) = 1 - exp(-x^2/(2*sigma^2)) , x >= 0
hist(x, prob = TRUE, main = expression(sigma == 3),ylim = c(0,0.3))
y <- seq(0, 12, .01)
lines(y, (y / sigma^2) 
      * exp( - y^{2} / (2 * sigma ^ 2)))
layout(1)
```

Above all, we can observe that the mode of the generated samples is close to the theoretical mode when the distribution is Rayleigh($\sigma$) distribution. 

# Question 2 (Exercise 3.11 in "Statistical Computing with R")

Generate a random sample of size $1000$ from a normal location mixture. The components of the mixture have $\mathcal{N}(0, 1)$ and $\mathcal{N}(3, 1)$ distributions with mixing probabilities $p_1$ and $p_2 = 1 − p_1$. Graph the histogram of the sample with density superimposed, for $p_1 = 0.75$. Repeat with different values for p1 and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures.

## Answer

### Analysis

Compare the methods for simulation of a convolution and a mixture of
normal variables. The notation $S = X_1 + X_2$ denotes the convolution of $X_1$ and $X_2$. The distribution of $S$ is normal with mean $\mu_1 + \mu_2 = 3$ and variance $\sigma_1^2+\sigma_2^2=2.$ Unlike the convolution above, the distribution of the mixture $X$ is distinctly non-normal; it is bimodal. 

To simulate the mixture: 

1. Generate an integer $k \in \{0, 1\}$ where $P(0) = p_0 ,P(1) = 1-P(0).$ 

2. If $k = 0$ deliver random x from $X_1 \sim \mathcal{N}(0, 1)$; if $k = 1$ deliver random x from $X_2 \sim \mathcal{N}(3, 1)$. 

### Examples

Now, graph the histogram of the sample with density superimposed, for $p_1 = 0.75$.

```{r}
rm(list=ls())
n <- 1000
X1 <- rnorm(n,0,sd=1)
X2 <- 3 + rnorm(n,0,sd=1)
p1 <- 0.75 # 0.50时有明显双峰
p2 <- 1 - p1
choice <- rbinom(n, 1, p1)
Z <- choice * X1 + (1-choice) * X2
par(mfrow=c(1,3))
hist(X1);hist(Z);hist(X2)
layout(1)
```

Since $X_1 \sim \mathcal{N}(0, 1)$ might make the existence of $X_2 \sim \mathcal{N}(3, 1)$ more obscure when $p_1 >> p_2$, and similar circumstance might happen when $p_1 << p_2$. So, when $p_1 \approx p_2$, such as $p_1 = p_2 = 0.5$, the histogram should be bimodal.

```{r,echo=FALSE}
rm(list=ls())
n <- 1000
X1 <- rnorm(n,0,sd=1)
X2 <- 3 + rnorm(n,0,sd=1)
p1 <- 0.50 
p2 <- 1 - p1
choice <- rbinom(n, 1, p1)
Z <- choice * X1 + (1-choice) * X2
par(mfrow=c(1,3))
hist(X1);hist(Z);hist(X2)
layout(1)
```

## Question 3 (Exercise 3.20 in "Statistical Computing with R")

A compound Poisson process is a stochastic process $\{X(t), t \geq 0\}$ that can be represented as the random sum $X(t)=\sum_{i=1}^{N(t)} Y_i, t \geq 0,$ where $\{N(t), t \geq 0\}$ is a Poisson process and $Y_1, Y_2, \ldots$ are i.i.d. and independent of $\{N(t), t \geq 0\}.$ Write a program to simulate a compound Poisson($\lambda$)–Gamma process ($Y$ has a $\Gamma$ distribution). Estimate the mean and the variance of $X(10)$ for several choices of the parameters and compare with the theoretical values.
Hint: Show that $E[X(t)]=\lambda t E\left[Y_1\right]$ and $\operatorname{Var}(X(t))=\lambda t E\left[Y_1^2\right].$

## Answer

### Analysis

A homogeneous Poisson process $\{N(t), t ≥ 0\}$ with rate $\lambda$ is a counting process, with independent increments, such that $N(0) = 0$ and $$ P(N(s+t)-N(s)=n)=\frac{e^{\lambda t}(\lambda t)^n}{n!}, \quad n \geq 0,\quad t, s>0. $$Thus, a homogeneous Poisson process has stationary increments and the number of events $N(t)$ in $[0, t]$ has the Poisson($\lambda t$) distribution. In the same time, $Y_1 \sim \Gamma(r,\beta)$. Note that the mean of $Y_1 \sim \Gamma(r,\beta)$ is $r / \beta$, the variance of $Y_1 \sim \Gamma(r,\beta)$ is $r / \beta^2$.

### Examples

We assume that the number of random samples is $1000$, time is $t = 10$. the parameters $\lambda = 1, r = 1, \beta = 1$. Computing the mean and variance of the random samples, and compare the values with the relatively theoretical values.

```{r}
rm(list=ls())
n <- 1000
t <- 10
lambda <- 1
r <- 1
beta <- 1
Nt <- rpois(n, lambda * t)
X <- rep(0,n)
for (i in 1:n) {
  Y <- rgamma(Nt[i], r, beta)
  X[i] <- sum(Y)
} # Now, X is the vector whose elements are the samples we need to use
tmean <- lambda * t * (r / beta)
tvariance <- lambda * t * ((r+1)*r / beta)

mean(X); tmean # tmean is the theoretical mean
var(X); tvariance # tvariance is the theoretical variance
```

The relative values of the random samples are close to the theoretical vales. 
# Homework-2024-09-23

## Question 1 (Exercise 5.4 in "Statistical Computing with R")

Write a function to compute a Monte Carlo estimate of the $Beta(3, 3)$ cdf, and use the function to estimate $F(x)$ for $x = 0.1, 0.2,\ldots, 0.9$. Compare the estimates with the values returned by the "pbeta" function in R.

## Answer

### Analysis

Generally, we can know the Beta function is a function whose formula is $$B(\alpha, \beta) = \int^1_0 x^{\alpha - 1}(1-x)^{\beta-1} dx,$$ There are additional formula for Beta function, that is $$B(\alpha, \beta) = \frac{\Gamma(\alpha) \times \Gamma(\beta)}{\Gamma(\alpha+\beta)}.$$ Under these formula, one could find out the p.d.f. of Beta distribution, that is $$f(x;\alpha,\beta) = \frac{1}{Beta(3,3)}x^{\alpha-1}(1-x)^{\beta-1}.$$ Because there is no explicit formula for the c.d.f. of Beta distribution without integral mode, one needs to use the Monte Carlo Method to compute the values in R. 

### Examples

We assume that the number of random samples is $1000$, the parameters $\alpha$ and $\beta$  are equal to 3, use the function to estimate F(x) for x = 0.1, 0.2,..., 0.9.

```{r}
rm(list=ls())
n <- 1000
X = 0.1*(1:10)
eY <- rep(0,10)
B = beta(3,3)
f <- function(x){
  (1/B) * x^2 * (1 - x)^2
}
for (i in 1:10) {
  u <- runif(n,0,X[i])
  g <- X[i] * f(u)
  eY[i] <- mean(g) #蒙特卡洛方法计算对应值
}
tY <- pbeta(X,3,3) #理论Beta分布函数对应值
par(mfrow = c(1,2))
plot(X,tY,type = "o",main = "理论值")
plot(X,eY,type = "o",main = "蒙特卡洛计算值")
layout(1)
```

The picture of the experiments of the Monte Carlo Method is close to the picture of the theoretical case.

## Question 2 (Exercise 5.9 in "Statistical Computing with R")

The Rayleigh density is $$f(x)=\frac{x}{\sigma^2} e^{-x^2 /\left(2 \sigma^2\right)}, \quad x \geq 0, \sigma>0.$$ Implement a function to generate samples from a Rayleigh(σ) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{\prime}}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent $X_1,X_2$?

## Answer

### Analysis

Because of the continuous p.d.f. of the distribution, we can use the method named " the inverse transform method" to generate random samples from some Rayleigh($\sigma$) distributions whose parameters $\sigma$ are  different and graph the relative histograms. Note that the c.d.f. of the distribution is $$F(x) = 1 - exp(-x^2/(2*\sigma^2)) ,\quad x \geq 0.$$ If $U_1$ and $U_2$ are unbiased estimators of $\theta$ and negatively correlated, $\frac{U_1 + U_2}{2}$ is also unbiased for $\theta$ but its variance is $$
\begin{aligned}
\operatorname{var}\left\{\left(U_1+U_2\right) / 2\right\} & =\left\{\operatorname{var}\left(U_1\right)+\operatorname{var}\left(U_2\right)+2 \operatorname{cov}\left(U_1, U_2\right)\right\} / 4 \\
& <\left\{\operatorname{var}\left(U_1\right)+\operatorname{var}\left(U_2\right)\right\} / 4.
\end{aligned}
$$Suppose $var(U_1) = var(U_2)$. If random numbers $X_1,\ldots,X_{m/2}\quad(i.i.d.)$ and $X_{(m+1)/2},\ldots,X_{m}\quad(i.i.d.)$ are used to calculate $U_1$ and $U_2$, and $U_1$ and $U_2$ are negatively correlated, then the variance of $T_1 = (U_1 + U_2)/2$ is smaller than the one based on
$X_1, . . . , X_{m/2}, X_{(m+1)/2}, . . . , X_m \quad (i.i.d.)$ , denoted by $T_2 \quad (var(T_1) = var(U_1)/2, var(T_2) < var(U_1)/2)$. The two estimates use the same number of random numbers.

### Examples

We assume that the number of random samples is $1000$, and compute the sample means and variances of the result of the experiments for $1000$ times. In the end, one should use the final mean and variance of the sample means and variances.

```{r}
rm(list=ls())
upper_value <- 5 #积分上限
sigma <- 1 #参数取值
Rayleigh <- function(x, R = 1000, antithetic = FALSE) {
  u <- runif(R/2)
  if (antithetic) v <- 1 - u else v <- runif(R/2)
  u <- c(u, v)
  g <- ((x*u) / sigma^2) * exp(-(x*u)^2 / (2*sigma^2)) # x*u ~ N(0,x)
  cdf <- mean(g)
  cdf
} # 1000个取值取均值
m <- 1000 # 1000次实验
Rayleigh1 <- Rayleigh2 <- numeric(m)
x <- 5
for (i in 1:m) {
  Rayleigh1[i] <- Rayleigh(x, R = 1000, antithetic = FALSE)#第一类，未对偶
  Rayleigh2[i] <- Rayleigh(x, R = 1000, antithetic = TRUE)#第二类，对偶
}
round(c(mean(Rayleigh1),mean(Rayleigh2)),4)
round(c(sd(Rayleigh1),sd(Rayleigh2),sd(Rayleigh2)/sd(Rayleigh1)),4)

```

The output results obtained by the Monte Carlo method of the two methods are approximately the same, and the variance of the data obtained by the latter is smaller than that obtained by the former.

## Question 3 (Exercise 5.13 in "Statistical Computing with R")

Find two importance functions $f_1 $ and $f_2$ that are supported on $(1, \infty)$ and are ‘close’ to $$
g(x)=\frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2}, \quad x>1.
$$ Which of your two importance functions should produce the smaller variance in estimating $\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x$ by importance sampling? Explain.

## Answer

### Analysis

Suppose $X$ is a random variable with density function $f(x)$, such that $f(x)>0$ on the set $\{x: g(x)>0\}$. Let $Y$ be the random variable $g(X) / f(X)$. Then $$
\int g(x) d x=\int \frac{g(x)}{f(x)} f(x) d x=E[Y].
$$ Estimate $\mathbb{E}[Y]$ by simple Monte Carlo integration.  The density $f(x)$ is called the importance function.

From now on, we use the function $$g(x) = \frac{1}{x^4} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2 x^2}} $$ to answer the relative questions because $g(x)$ is a function such that $$\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x = 
\int_0^1 \frac{1}{t^4} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2 t^2}} dt$$.

### Examples

One can choose $f_1(x) = e^{-x}$ and $f_2(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ to compute and use function $f_0(x) \equiv 1$ to compare.

```{r}
# 调整方差的方法: importance function.
rm(list=ls())
m <- 1000 #调整方差后的蒙特卡洛方法实验次数
MC_times <- 1000 #每次实验的样本量
cdf0 <- cdf1 <- cdf2 <- numeric(MC_times)
theta.hat <- se <- numeric(3) # f0 = 1, f1, f2.

g <- function(x) {
  (1/(sqrt(2*pi)*x^4)) * exp(-1/(2*x^2)) * (x >= 0) * (x <= 1)
}

f2 <- function(x){
  (1/sqrt(2*pi)) *exp(- x^2 / 2)
} # 标准正态分布的p.d.f.

for (i in 1:MC_times) {
  x <- runif(m) #using f0
  fg <- g(x)
  cdf0[i] <- mean(fg) 
}
theta.hat[1] <- mean(cdf0) # f0下实验结果均值
se[1] <- sd(cdf0)# f0下实验结果方差

for (i in 1:MC_times) {
  x <- rexp(m, 1) #using f1
  fg <- g(x) / exp(-x)
  cdf1[i] <- mean(fg)
}
theta.hat[2] <- mean(cdf1)# f1下实验结果均值
se[2] <- sd(cdf1)# f1下实验结果方差

for (i in 1:MC_times) {
  x <- rnorm(n = m,mean = 0,sd = 1) #using f2
  fg <- g(x) / f2(x)
  cdf2[i] <- mean(fg)
}
theta.hat[3] <- mean(cdf2)# f1下实验结果均值
se[3] <- sd(cdf2)# f2下实验结果方差

round(theta.hat,4)

X = (1:1000)*.001
Y = g(X)
Y0 = rep(1,length(X))
Y1 = exp(-X)
Y2 = f2(X)
plot(X,Y,type = "l",ylim = c(0,1.4),main = "f(x)与g(x)的函数图像",col = 1)
lines(X,Y0,type = "l",col = 2)
lines(X,Y1,type = "l",col = 3)
lines(X,Y2,type = "l",col = 4)
legend(x = "topleft",
       legend = c("g(x)","f(x)=1","f(x)为参数为1指数函数",
                  "f(x)为标准正态分布"),
       col = c(1,2,3,4),lty = rep(1,4), cex = 0.6)
```

Maybe the functions $f_1,f_2$ one chosen are not 'close' to $g(x)$, the results of the experiments are that the means are almost close, but the variances are much larger. 

## Question 4 (Monte Carlo experiment)

1. For $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1,\ldots, n$.

2. Calculate computation time averaged over $100$ simulations,
denoted by $a_n$.

3. Regress $a_n$ on $t_n := n log(n)$, and graphically show the results (scatter plot and regression line).

## Answer

### Analysis

The quicksort method is an efficient sorting algorithm that results in a completely ordered sequence by continuously splitting a sequence into two subsequences and sorting each subsequence. The time complexity of quicksort is $O(nlogn)$ in the average case and $O(n^2 )$ in the worst case, but the latter rarely happens and can be avoided with a few tricks.

The basic steps for quicksort are as follows:

1. Select an element from the sequence as the pivot, which can be the first element, a random element, a median, etc.
2. Define two pointers, i and j, pointing to the beginning and end of the sequence.
3. Move j from right to left, find the first element less than or equal to the pivot, swap it with the element to which i refers, and then move i one place to the right.
4. Move i from left to right, find the first element greater than or equal to the pivot, swap it with the element that j refers to, and then move j one place to the left.
5. Repeat steps 3 and 4 until i and j meet.
6. Return the position of i, which serves as the final position of the pivot.

### Examples

```{r}
n = 20 # 总共可对n*1e4数目的数据排序
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}#??????
} # 快速排序法
An = rep(0,n)
Tn = An
for (i in 1:n) {
    test<-sample(x = 1:(n*1e4), size = i*1e4)
    An[i] <- system.time(quick_sort(test))[1] # 快速排序法所需时间
    Tn[i] <- (i*1e4)*log((i*1e4)) # nlogn
}
regression_result <- lm(An~Tn)
summary(regression_result)
plot(Tn,An,main = "scatter plot and regression line",xlab = "nlog(n)",ylab = "computation time")
En <- regression_result$coefficients[1] + regression_result$coefficients[2] * Tn
lines(Tn,En) # 回归线
```

The regression results and the images show a strong linear correlation between $a_n$ and $t_n$.

# Homework-2024-09-30

## Note

From this paper on, in complex simulation studies, use at least three separate functions for:

(1) data generation;

(2) statistical inference (ready for real data analysis);

(3) result reporting.

When scoring the exercises (TA):

(1) Check if there are at least three functions (if not, deduct 1 point).

(2) Check if memory is cleared up before calling each function (if not, deduct 1 more point).

## Question 1 (Exercise 6.6 in "Statistical Computing with R")

Estimate the $0.025, 0.05, 0.95$ and $0.975$ quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_1} \approx \mathcal{N}(0, 6/n)$.

## Answer

### Analysis

The skewness $\sqrt{\beta_1}$ of a random variable $X$ is defined by $$
\sqrt{\beta_1}=\frac{E\left[\left(X-\mu_X\right)\right]^3}{\sigma_X^3},
$$ where $\mu_X=E[X]$ and $\sigma_X^2=\operatorname{Var}(X)$. The sample coefficient of skewness is denoted by $\sqrt{b_1}$, and defined as $$
\sqrt{b_1}=\frac{\frac{1}{n} \sum_{i=1}^n\left(X_i-\bar{X}\right)^3}{\left(\frac{1}{n} \sum_{i=1}^n\left(X_i-\bar{X}\right)^2\right)^{3 / 2}}.
$$ If the distribution of $X$ is normal, then $\sqrt{b_1}$ is asymptotically normal with mean $0$ and variance $6/n$.

Exactly, The variance of the $q$ sample quantile is $$
\operatorname{Var}\left(\hat{x}_q\right)=\frac{q(1-q)}{n f\left(x_q\right)^2},$$ where $f$ is the density of the sampled distribution.

### Experiments

(1) data generate:

```{r}
rm(list=ls())
n_skewness = 1000; n_MC = 10000
# 生成数据的函数
generate_data <- function(n_skewness = 1000, n_MC = 10000){
  skewness <- numeric(n_skewness)
  for (i in 1:n_skewness) {
    e <- rnorm(n_MC)
    skewness[i] <- mean(((e-mean(e))/sd(e))^3)
  }
  return(skewness)
}
e <- generate_data(n_skewness,n_MC)
qt = c(0.025,0.05,0.95,0.975)
#求解分位数
Quantiles <- function(unknown_data, qt){
  quantile(unknown_data,qt)
}
query_x <- Quantiles(e,qt)
round(query_x,4)
```

(2) Compute the standard error of the estimates: (Note: use the kernel density estimation method with "Guassian's distribution")

```{r}
SEE <- function(unknown_data,query_x,n_skewness){
  # 使用核密度估计估计密度函数
  density_est <- density(unknown_data)
  # 在密度估计的自变量值x中查找最接近的值
  L <- length(query_x)
  nearest_index <- numeric(L)
  for (i in 1:L) {
    nearest_index[i] <- which.min(abs(density_est$x - query_x[i]))
  }
  # 获取对应自变量值的密度值
  corresponding_density <- density_est$y[nearest_index]
  # 输出估计值的标准差
  SEE_result <- numeric(L)
  for (j in 1:L) {
    VAR_q <- qt[j]*(1-qt[j]) / (n_skewness * corresponding_density[j]^2)
    SEE_result[j] <- sqrt(VAR_q)
  }
  return(SEE_result)
}
SEE_result <- SEE(e,query_x,n_skewness)
round(SEE_result,4)
```

(3) Compare the estimated quantiles with the quantiles of the large sample approximation by a simple table:

```{r,eval=FALSE}
library(xtable)#引用xtable包
Comparation <- function(query_x , qt, n_MC){
  # 渐进分布的分位数
  a <- rnorm(n_MC,mean = 0,sd = sqrt(6/n_MC))
  a_query_x <- quantile(a,qt)
  Table <- rbind(query_x,a_query_x)
  Table <- as.data.frame(Table)
  colnames(Table) <- qt
  row.names(Table) <- c("estimated quantiles",
                        "the quantiles of the large sample approximation")
  xtable(Table,digits = 4,caption = "Quantiles")
}
Comparation(query_x, qt, n_MC) #表格的latex代码
```

Table example:

```{=tex}
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & 0.025 & 0.05 & 0.95 & 0.975 \\ 
  \hline
estimated quantiles & -0.0453 & -0.0397 & 0.0395 & 0.0464 \\ 
  the quantiles of the large sample approximation & -0.0472 & -0.0394 & 0.0412 & 0.0484 \\ 
   \hline
\end{tabular}
\caption{Quantiles} 
\end{table}
```
## Question 2 (Exercise 6.B in "Statistical Computing with R")

Tests for association based on Pearson product moment correlation $\rho$, Spearman’ s rank correlation coefficient $\rho_s$, or Kendall’s coefficient $\tau$, are implemented in cor.test. Show (empirically) that the nonparametric tests based on $\rho_s$ or $\tau$ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate distribution $(X, Y )$ such that $X$ and $Y$ are dependent) such that at least one of the nonparametric tests have better empirical power than the correlation test against this alternative.

## Answer

### Analysis

If there are two variables: $X$ and $Y$, the meaning of the final calculated correlation coefficient can be understood as follows:

(1) When the correlation coefficient is $0$, there is no relationship between $X$ and $Y$.

(2) When the value of $X$ increases (decreases) and the value of $Y$ increases (decreases), the two variables are positively correlated, and the correlation coefficient is between $0.00$ and $1.00$.

(3) When the value of $X$ increases (decreases) and the value of $Y$ decreases (increases), the two variables are negatively correlated, and the correlation coefficient is between $-1.00$ and $0.00$.

Assuming that there are two variables X and Y, then the Pearson product moment correlation between the two variables can be calculated by the following formula:$$
\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_X \sigma_Y}=\frac{E\left(\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right)}{\sigma_X \sigma_Y}=\frac{E(X Y)-E(X) E(Y)}{\sqrt{E\left(X^2\right)-E^2(X)} \sqrt{E\left(Y^2\right)-E^2(Y)}}
$$
The correlation coefficient is defined when the standard deviation of both variables is not zero, and the Pearson product moment correlation is applied to:

(1) There is a linear relationship between the two variables, and they are both continuous data.

(2) The population of the two variables is normally distributed, or nearly normal, with a unimodal distribution.

(3) The observations of the two variables are paired, and each pair of observations is independent of each other.

The Spearman rank correlation coefficient between the random variables X and Y can be calculated from the ranking set x,y or the ranking difference set d. It is calculated from the set of rankings x and y (the correlation coefficient of Spearman's rank is also considered to be the Pearson product moment correlation of the two immediate variables that pass through the ranking, and the following is actually the Pearson product moment correlation of x, y is actually calculated): $$
\rho_s=\frac{\sum_{i=1}^N\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sqrt{\sum_{i=1}^N\left(x_i-\bar{x}\right)^2 \sum_{i=1}^N\left(y_i-\bar{y}\right)^2}}
$$

Suppose that the two random variables are $X$ and $Y$ (which can also be regarded as two sets), and the number of elements in them is $N$, and the $i\ (1 \leq i \leq N)$ values of the two subsequent variables are represented by $X_i$ and $Y_i$ respectively. The corresponding elements in $X$ and $Y$ form a set of element pairs $XY$. When any two elements $(X_i, Y_i)$ and $(X_j, Y_j)$ in the set $XY$ are ranked the same, they are considered to be consistent, whereas they are considered to be inconsistent.

Formula for calculating Kendall correlation coefficient: $$
\tau = \frac{C-D}{\frac{1}{2} N(N-1)}.
$$ where $C$ indicates that there is a consistent logarithm of elements in $XY$ (two elements are a pair); $D$ indicates that there are inconsistent logarithms of elements in $XY$.

### Experiments

(1) data generate 1: (the sampled distribution is bivariate normal)

```{r}
rm(list=ls())
library(MASS)

# set seed and create data vectors
set.seed(12345)
sample_size <- 1000 
sample_meanvector <- c(0, 0)
sample_covariance_matrix <- matrix(c(1,0.5,0.5,1),
                                   ncol = 2)
# create bivariate normal distribution
sample_distribution <- mvrnorm(n = sample_size,
                               mu = sample_meanvector, 
                               Sigma = sample_covariance_matrix)
```

(2) data generate 2: (the sampled distribution is $F(x) = x^3,\ 0 \leq x \leq 1$)

```{r}
alternative <- function(sample_size){
  u <- runif(sample_size)
  x <- u^{1/3} # F(x) = x^3, 0<=x<=1
  return(cbind(u,x))
}
X <- alternative(sample_size)
```

(3) compute kinds of correlation coefficients: 

```{r}
#correlation coefficient
Comparate_cor <- function(sample_distribution){
  COR <- numeric(3)
  COR[1] <- cor.test(sample_distribution[,1], sample_distribution[,2],
                method = "pearson")$estimate
  COR[2] <- cor.test(sample_distribution[,1], sample_distribution[,2],
                     method = "spearman")$estimate
  COR[3] <- cor.test(sample_distribution[,1], sample_distribution[,2],
                     method = "kendall")$estimate
  COR
}
# the result of experiments
# the sampled distribution is bivariate normal
COR1 <- Comparate_cor(sample_distribution)
# the sampled distribution is F(x) = x^3, 0<=x<=1
COR2 <- Comparate_cor(X)
# turn: "pearson" -> "spearman" -> "kendall"
round(COR1,4)
round(COR2,4)
```

### Question 3 (Homework in class)

If we obtain the powers for two methods under a particular simulation setting with $10,000$ experiments: say, $0.651$ for one method and $0.676$ for another method. We want to know if the powers are different at $0.05$ level.

(1) What is the corresponding hypothesis test problem?

(2) What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

(3) Please provide the least necessary information for hypothesis testing.

### Answer

(1) The hypothesis test problem is $$H_0: p_1 = p_2 \leftrightarrow H_1: p_1 \neq p_2.$$ the powers for one method is $p_1$, for the other method is $p_2$.

(2) Can't use: 

\begin{itemize}
\item[a] Z-test: the Z-test requires the following: the two samples are independent.
\item[b] Two-sample t-test: the test requires the following: the two samples are independent.
\end{itemize}

Can use: 

\begin{itemize}
\item[a] Paired-t test: it is typically used in the following situations: 1. Measurements of the same individual under different conditions: e.g. measurements of the same individual before and after treatment. 2. paired designs: e.g. measurements of the left and right hands of the same object. Note: The paired t-test can be used for related samples, where two samples appear in pairs and are related to each other. The paired t-test calculates the difference for each pair of samples, and then calculates the mean and standard deviation for those differences, which in turn calculates the t-statistic.
\item[b] McNemar test: the McNemar test is a statistical test used to pair nominal data. It is applied to a 2 × 2 contingency table with dichotomous characteristics, with matching subject pairs, to determine if the marginal frequencies of rows and columns are equal (i.e., if there is "marginal homogeneity"). 
\end{itemize}

(3) The note as below:

For all: The number of observations of the two samples is the same, and the order of the observations of the two samples cannot be changed arbitrarily.

For paired-t test: The dependent variable is measured at an incremental level, such as a ratio or interval. indeed, The argument must consist of two related groups or matching pairs.

For McNemar test: the McNemar change significance test requires that the observed values of the two groups of samples to be tested are dichotomous data, and when the observed values of the two paired samples are not binary data, the former test method cannot be used, and the sign test of the two paired samples can be used.

# Homework-2024-10-14

## Question 1 (Homework in class)

Of $N = 1000$ hypotheses, $950$ are null and $50$ are alternative.The p-value under any null hypothesis is uniformly distributed(use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter $0.1$ and $1$ (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted p-values. Calculate FWER, FDR, and TPR under nominal level $\alpha = 0.1$ for each of the two adjustment methods based on $m = 10000$ simulation replicates. You should output the $6$ numbers to a $3 \times 2$ table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR).Comment the results.

## Answer

### Analysis

\begin{table}[ht]
\centering
\begin{tabular}{l|l|l|l}
\hline
& H0 is true & Ha is true & Total\\
\hline
Positive(reject H0) & V (FP) & S (TP) & R\\
\hline
Negative (accept H0) & U (TN) & T (FN) & m-R\\
\hline
Total & m0 & m-m0 & m\\
\hline
\end{tabular}
\end{table}

1. False positive rate (FPR) = $V/m_0$.

2. True positive rate (TPR) = $S/(m-m0)$.

3. False negative rate (FNR) = $T/(m-m0)$.

4. True negative rate (TNR) = $U/m_0$.

5. False discovery rate (FDR) = $V/R$.

6. True discovery rate (TDR) = $S/R$.

7. Family-wise error rate (FWER) = $P(\mbox{reject any null hypothesis} \mid \text{all }N\mbox{ null hypotheses are true})$.

### Experiments
```{r}
rm(list=ls())
m <- 1e4
hypotheses_null_num <- 950
hypotheses_alternative_num <- 50
N <- hypotheses_null_num + hypotheses_alternative_num
labels_hypotheses <- numeric(N)
labels_hypotheses[1:hypotheses_null_num] <- 
  rep(0,hypotheses_null_num)
labels_hypotheses[(hypotheses_null_num+1):N] <-
  rep(1,hypotheses_alternative_num)
data_generate <- function(n1,n2,m){
  Data <- matrix(0, nrow = (n1+n2), ncol = m)
  for (i in 1:m) {
    data_null <- runif(n1)
    data_alternative <- rbeta(n2,0.1,1)
    data <- numeric(n1+n2)
    data[1:n1] <- data_null
    data[(n1+1):(n1+n2)] <- data_alternative
    Data[,i] <- data
  }
  return(Data)
}
Data <- data_generate(hypotheses_null_num,
                      hypotheses_alternative_num,m)
Bonferroni <- function(data,m,N){
  Data <- matrix(0, nrow = N, ncol = m)
  for (i in 1:m) {
    Data[,i] <- p.adjust(data[,i],method = 'bonferroni')
  }
  return(Data)
}
BH <- function(data,m,N){
  Data <- matrix(0, nrow = N, ncol = m)
  for (i in 1:m) {
    Data[,i] <- p.adjust(data[,i],method = 'BH')
  }
  return(Data)
}
alpha <- 0.1

Data.Bonferroni <- Bonferroni(Data,m,N)
Data.BH <- BH(Data,m,N)
FDR.Bonferroni <- FDR.BH <- TPR.Bonferroni <- TPR.BH <-
  FWER.Bonferroni <- FWER.BH <- numeric(m)
for (i in 1:m) {
  R <- sum(Data.Bonferroni[,i] <= alpha)
  V <- sum(Data.Bonferroni[1:hypotheses_null_num,i] <= alpha)
  S <- R-V
  U <- sum(Data.Bonferroni[(1+hypotheses_null_num):N,i] <= alpha)
  TT <- N-R-U
  FDR.Bonferroni[i] <- V/R
  TPR.Bonferroni[i] <- S/hypotheses_alternative_num
  if (V>0) {
    FWER.Bonferroni[i] <- 1
  } else {
    FWER.Bonferroni[i] <- 0
  }
}

for (i in 1:m) {
  R <- sum(Data.BH[,i] <= alpha)
  V <- sum(Data.BH[1:hypotheses_null_num,i] <= alpha)
  S <- R-V
  U <- sum(Data.BH[(1+hypotheses_null_num):N,i] <= alpha)
  TT <- N-R-U
  FDR.BH[i] <- V/R
  TPR.BH[i] <- S/hypotheses_alternative_num
  if (V>0) {
    FWER.BH[i] <- 1
  } else {
    FWER.BH[i] <- 0
  }
}

result <- matrix(0,nrow = 2,ncol = 3)
result[1,] <- c(mean(FWER.Bonferroni),mean(FDR.Bonferroni),mean(TPR.Bonferroni))
result[2,] <- c(mean(FWER.BH),mean(FDR.BH),mean(TPR.BH))
result <- as.data.frame(result,row.names = c("Bonferroni","BH"))
colnames(result) <- c("FWER","FDR","TPR")
result
```

## Question 2 (Exercise 7.4 in "Statistical Computing with R")

Refer to the air-conditioning data set aircondit provided in the boot package. The $12$ observations are the times in hours between failures of air-conditioning equipment [$63$, Example $1.1$]: $$3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.$$ Assume that the times between failures follow an exponential model $\operatorname{Exp}(\lambda)$. Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

## Answer

### Analysis

Suppose the population has the exponential distribution with rate $\lambda$, then the $MLE$ of $\lambda$ is $\hat{\lambda} = \frac{1}{\bar{X}}$, where $\bar{X}$ is the sample mean.

### Experiments

```{r}
library(boot)
rm(list=ls())
data <- as.array(aircondit$hours)
MLE <- function(x){
  return(1/mean(x))
} 
SIZE <- length(data)
R <- 1e4
Mboot <- replicate(R, expr = {
  y <- sample(data, size = SIZE, replace = TRUE)
  MLE(y)
})
round(c(bias = mean(Mboot) - MLE(data),se = sd(Mboot)),3)
```

## Question 3 (Exercise 7.5 in "Statistical Computing with R")

Refer to Exercise $7.4$. Compute $95\%$ bootstrap confidence intervals for the
mean time between failures $\frac{1}{\lambda}$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer

### Analysis

1. The standard bootstrap CI based on asymptotic normality ([\color{blue}normal]()):$$(\hat\theta-z_{1-\alpha/2}\hat{se}(\hat\theta),\hat\theta-z_{\alpha/2}\hat{se}(\hat\theta)).$$

2. The basic bootstrap CI based on the large sample property ([\color{blue}basic]()):The resulting $(1-\alpha)$-CI is $$(2\hat\theta-\hat\theta^*_{1-\alpha/2},2\hat\theta-\hat\theta^*_{\alpha/2}).$$where $\hat\theta^*_a$ is the $a$-quantile of $\hat\theta^*$.

3. Percentile CI ([\color{blue}percent]()) by assuming $\hat\theta^*|\textbf{X}$ and $\hat\theta$ have approximately the same distribution: $$(\hat\theta^*_{\alpha/2},\hat\theta^*_{1-\alpha/2}).$$

4. Bias-corrected and accelerated CI  ([\color{blue}BCa]()):
        $$(\hat\theta^*_{\alpha_1},\hat\theta^*_{\alpha_2})$$
        $$\alpha_1=\Phi(\hat z_0+\frac{\hat z_0+z_{\alpha/2}}{1-\hat a (\hat z_0+z_{\alpha/2})}), \alpha_2=\Phi(\hat z_0+\frac{\hat z_0+z_{1-\alpha/2}}{1-\hat a (\hat z_0+z_{1-\alpha/2})})$$
        $$\hat z_0=\Phi^{-1}\left(\frac1B\sum_{b=1}^BI(\hat\theta^{(b)}<\hat\theta)\right), \hat a=\frac{\sum_{i=1}^n(\bar{\theta}_{(\cdot)}-\theta_{i})^3} {6\sum_{i=1}^n((\bar{\theta}_{(\cdot)}-\theta_{i})^2)^{3/2}}$$

### Experiments

```{r}
library(boot)
rm(list=ls())
data <- as.array(aircondit$hours)
SIZE <- length(data)
m <- 1e2
boot.MLE <- function(x,i) mean(x[i])
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2)
for(i in 1:m){
  de <- boot(data,statistic=boot.MLE, R = 999)
  ci <- boot.ci(de,conf = 0.95,
                type=c("norm","basic","perc","bca")) # level: 0.95
  ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5];ci.bca[i,]<-ci$bca[4:5]
}
# The standard bootstrap CI based on asymptotic normality
round(c(mean(ci.norm[,1]),mean(ci.norm[,2])),4) 
round(c(mean(ci.basic[,1]),mean(ci.basic[,2])),4) # The basic bootstrap CI
round(c(mean(ci.perc[,1]),mean(ci.perc[,2])),4) # Percentile CI
round(c(mean(ci.bca[,1]),mean(ci.bca[,2])),4) # Bias-corrected and accelerated CI
```
Different methods may treat extreme values or the shape of the distribution differently, leading to differences in confidence intervals.

# Homework-2024-10-21

## Question 1 (Exercise 7.7-7.8 in "Statistical Computing with R")

### 7.7

Refer to Exercise $7.6$. Efron and Tibshirani discuss the following example. The five-dimensional scores data have a $5 \times 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_1>\cdots>\lambda_5$. In principal components analysis,
$$
\theta=\frac{\lambda_1}{\sum_{j=1}^5 \lambda_j}
$$
measures the proportion of variance explained by the first principal component. Let $\hat{\lambda}_1>\cdots>\hat{\lambda}_5$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the $MLE$ of $\Sigma$. Compute the sample estimate
$$
\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{j=1}^5 \hat{\lambda}_j}
$$
of $\theta$. Use bootstrap to estimate the bias and standard error of $\hat{\theta}$.

### 7.8

Refer to Exercise $7.7$. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

## Answer

### Analysis

From exercise $7.6$, Efron and Tibshirani discuss the scor (bootstrap) test score data on $88$ students who took examinations in five subjects. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores $\left(x_{i 1}, \ldots, x_{i 5}\right)$ for the $i^{t h}$ student.

### Experiments

```{r}
rm(list=ls())
library(bootstrap)
attach(scor)
head(scor)
B <- 1e3; set.seed(12345)
cor_matrix <- cor(scor)
straight_T <- function(cor_matrix){
  ev <- eigen(cor_matrix)
  max(ev$val[1]) / sum(ev$val)
} # 直接求出的参数估计

bootstrap_data <- function(data){
  x <- 1:nrow(data)
  xstar <- sample(x,replace=TRUE)
  ndata <- matrix(rep(0,nrow(data)*ncol(data)),nrow = nrow(data),ncol = ncol(data))
  for (i in 1:nrow(data)) ndata[i,] <- as.numeric(data[xstar[i],])
  ndata <- as.data.frame(ndata)
  colnames(ndata) <- colnames(data)
  return(ndata)
} # 重排数据生成
bootstrap_T <- function(data){
  total <- numeric(B)
  for(b in 1:B){
    ndata <- bootstrap_data(data)
    cor_matrix <- cor(ndata)
    ev <- eigen(cor_matrix)
    total[b] <- max(ev$val[1]) / sum(ev$val)
  }
  total
} # bootstrap方法求出的theta估计

jackknife_T <- function(data){
  total <- numeric(nrow(data))
  for (b in 1:nrow(data)) {
    ndata <- data[(1:nrow(data))[-b],]
    cor_matrix <- cor(ndata)
    ev <- eigen(cor_matrix)
    total[b] <- max(ev$val[1]) / sum(ev$val)
  }
  total
} # jackknife方法求出的theta估计

n <- nrow(scor)
theta <- straight_T(cor_matrix)
theta_bootstrap <- bootstrap_T(scor)
theta_jackknife <- jackknife_T(scor)
mean_jackknife <- rep(mean(theta_jackknife),nrow(scor))
se_jackknife <- (n-1)*mean((theta_jackknife-mean_jackknife)^2)
round(c(original=theta,bias.bootstrap=mean(theta_bootstrap)-theta,
        se.bootstrap=sd(theta_bootstrap),
        bias.jackknife=(n-1)*(mean(theta_jackknife)-theta),
        se.jackknife=se_jackknife),4)
detach(scor)
```

## Question 2 (Exercise 7.10 in "Statistical Computing with R")

In Example $7.18$ , leave-one-out ( $n$-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the $Log-Log$ model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$ ?

## Answer

### Experiments

```{r}
rm(list=ls())
library(DAAG); attach(ironslag)
a <- seq(10, 40, .1) #sequence for plotting fits

# 判据：调整的R方
L1 <- lm(magnetic ~ chemical) # 记为模型1
L2 <- lm(magnetic ~ chemical + I(chemical^2))# 记为模型2
L3 <- lm(magnetic ~ chemical + I(chemical^2) + I(chemical^3))# 记为模型3
L4 <- lm(log(magnetic) ~ log(chemical))# 记为模型4
summary1 <- summary(L1)
summary2 <- summary(L2)
summary3 <- summary(L3)
summary4 <- summary(L4)
ADJ.R.squared_result <- c(summary1$adj.r.squared,summary2$adj.r.squared,
               summary3$adj.r.squared,summary4$adj.r.squared)
which.min(ADJ.R.squared_result)

# for n-fold cross validation
# fit models on leave-one-out samples
CV <- function(M,C){
  n <- length(M) #in DAAG ironslag
  e1 <- e2 <- e3 <- e4 <- numeric(n)
  for (k in 1:n) {
    y <- M[-k]
    x <- C[-k]
    J1 <- lm(y ~ x)# 记为模型1
    yhat1 <- J1$coef[1] + J1$coef[2] * C[k]
    e1[k] <- M[k] - yhat1
    J2 <- lm(y ~ x + I(x^2))# 记为模型2
    yhat2 <- J2$coef[1] + J2$coef[2] * C[k] +
      J2$coef[3] * C[k]^2
    e2[k] <- M[k] - yhat2
    J3 <- lm(y ~ x + I(x^2) + I(x^3))# 记为模型3
    logyhat3 <- J3$coef[1] + J3$coef[2] * C[k]
    yhat3 <- exp(logyhat3)
    e3[k] <- M[k] - yhat3
    J4 <- lm(log(y) ~ log(x))# 记为模型4
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(C[k])
    yhat4 <- exp(logyhat4)
    e4[k] <- M[k] - yhat4
  }
  result <- c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
  result
}

CV_result <- CV(M = magnetic, C = chemical)
which.min(CV_result)
```
For all, Log-Log model is selected according to maximum adjusted $R^2$; quadratic polynomial model is selected by the cross validation procedure.

## Question 3 (Exercise 8.1 in "Statistical Computing with R")

Implement the two-sample Cramér-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples $8.1$ and $8.2$.

## Answer

### Analysis

The Cramér-von Mises test is used to test whether a column of data has the same distribution as another group of data. It is a nonparametric test method. The one-sample CVM test tests whether the unknown distribution comes from a parametric distribution, such as a normal distribution. If you want to compare two groups of data with unknown distributions, you need a two-sample CVM test.

The one-sample CVM test: $$
\omega^2=\int_{-\infty}^{\infty}\left[F_N(x)-F(x)\right]^2 d F(x),
$$ two-sample CVM test: $$
T=[N M /(N+M)] \int_{-\infty}^{\infty}\left[F_N(x)-G_M(x)\right]^2 d H_{N+M}(x).
$$

### Experiments

The data from examples  $8.1$ and $8.2$ are same. 
```{r}
rm(list=ls())
library(twosamples)
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))

R <- 999 #number of replicates
z <- c(x, y) #pooled sample
K <- 1:26
reps <- numeric(R) #storage for replicates
t0 <- cvm_test(x,y)[1]
for (i in 1:R) {
  #generate indices k for the first sample
  k <- sample(K, size = 14, replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  reps[i] <- cvm_test(x1, y1)[1]
}
p <- mean(c(t0, reps) <= t0)
detach(chickwts)
round(p,4)
```
p is much larger than usual value 0.05, so the null hypothesis is rejected. The data are not for same distribution.

## Question 4 (Exercise 8.2 in "Statistical Computing with R")

Implement the bivariate Spearman rank correlation test for independence as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

## Answer

### Analysis

The bivariate Spearman rank correlation test for independence was introduced in Homework-2024-09-31, the relative information would be omitted here.

### Experiments

```{r}
rm(list=ls())
data <- as.data.frame(state.x77)
x <- data$Income
y <- data$Illiteracy
R <- 999 #number of replicates
n <- length(x)
reps <- numeric(R) #storage for replicates
result0 <- cor.test(x,y,method = "spearman",exact=FALSE)
t0 <- result0$p.value

for (i in 1:R) {
  #generate indices k for the first sample
  y1 <- sample(y, size = n, replace = FALSE) #complement of x1
  result <- cor.test(x,y1,method = "spearman",exact=FALSE)
  reps[i] <- result$p.value
}
p <- mean(c(t0, reps) >= t0)
round(p,4)
```
p is much larger than usual value 0.05, so the alternative hypothesis is rejected. the two kinds of data from state.x77 are not independent.

# Homework-2024-10-28

## Question 1 (Homework in class)

### Algorithm (continuous situation)
    
+ Target pdf: $f(x)$.
    
+ Replace $i$ and $j$ with $s$ and $r$.
    
+ Proposal distribution (pdf): $g(r|s)$.
    
+ Acceptance probability: $\alpha(s,r)=\min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}$.
    
+ Transition kernel (mixture distribution): $K(r,s)=I(s\not=r)\alpha(r,s)g(s|r)+I(s=r)[1-\int \alpha(r,s)g(s|r)]$.
    
+ Stationarity: $K(s,r)f(s)=K(r,s)f(r)$.

proof: the property of the stationarity.

## Answer

1. $s=r$, the original equation is right naturally.

2. $s \neq r$: 

\begin{equation*}
\begin{aligned}
K(s,r)f(s) & = \alpha(s,r)g(r|s)f(s) \\
& = (1 \wedge \frac{f(r)g(s|r)}{f(s)g(r|s)})f(s)g(r|s) \\
& = (f(s) \wedge \frac{f(r)g(s|r)}{g(r|s)})g(r|s) \\
& = (f(r) \wedge \frac{f(s)g(r|s)}{g(s|r)})g(r|s) \\
& = f(r)g(r|s)(1 \wedge \frac{f(s)g(r|s)}{f(r)g(s|r)}) \\
& = f(r)K(r,s) \\
\end{aligned}
\end{equation*}

## Question 2 (Exercise 9.3 in "Statistical Computing with R")

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with $\mathrm{df}=1$ ). Recall that a $\operatorname{Cauchy}(\theta, \eta)$ distribution has density function

$$
f(x)=\frac{1}{\theta \pi\left(1+[(x-\eta) / \theta]^2\right)}, \quad-\infty<x<\infty, \theta>0 .
$$


The standard Cauchy has the $\operatorname{Cauchy}(\theta=1, \eta=0)$ density. (Note that the standard Cauchy density is equal to the Student $t$ density with one degree of freedom.)

## Answer

### Analysis

The algorithm (continuous situation) is above. The standard Cauchy density is equal to the Student $t$ density with one degree of freedom.

### Experiments

```{r}
rm(list=ls())
set.seed(12345)
f <- function(x, theta,eta) {
  num <- theta * pi * (1 + ((x - eta)/theta)^2)
  1/num
} # theta > 0
m <- 10000
theta <- 1
eta <- 0
# 提议分布为N(X,sigma^2)
sigma <- 4
x <- numeric(m)
x[1] <- rnorm(n = 1,mean = 0,sd = sigma) 
k <- 0
u <- runif(m)
for (i in 2:m) {
  xt <- x[i-1]
  y <- rnorm(n = 1,mean = xt,sd = sigma)
  num <- f(y, theta, eta) * dnorm(xt,mean = y,sd = sigma)
  den <- f(xt, theta, eta) * dnorm(y,mean = xt,sd = sigma)
  if (u[i] <= num/den){
    x[i] <- y
  } else {
    x[i] <- xt
    k <- k+1     #y is rejected
  }
}
b <- 1001
y <- x[b:m]
Q <- quantile(y, probs=seq(0.1,1, by=0.1))
QC <- qt(df = 1,p = seq(0.1,1, by=0.1))
round(Q,3)
round(QC,3)
```

## Question 3 (Exercise 9.8 in "Statistical Computing with R")

This example appears in [40]. Consider the bivariate density

$$
f(x, y) \propto\binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1}, \quad x=0,1, \ldots, n, 0 \leq y \leq 1
$$


It can be shown (see e.g. [23]) that for fixed $a, b, n$, the conditional distributions are $\operatorname{Binomial}(n, y)$ and $\operatorname{Beta}(x+a, n-x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.

## Answer

### Analysis

### About Gibbs sampler

+ Gibbs is named by Geman and Geman (1984) in the application to the analysis of Gibbs lattice distributions

+ Gibbs is ideal for multivariate distributions if it is easy to sample from each univariate conditional density

### Algorithm

+ $X=(X_1,\ldots,X_d)$: random vector of interest

+ $X_{(-j)}=(X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_d)$

+ Suppose it is easy to generate random numbers from the conditional distribution $f(X_j|X_{(-j)})$.

### Experiments

```{r}
rm(list=ls())
#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
a <- 1
b <- 1
n <- 10
###### generate the chain #####

X[1, ] <- c(n/2, 0.5) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
X[i, 1] <- rbinom(1,n,x2)
x1 <- X[i, 1]
X[i, 2] <- rbeta(1,x1+a,n-x1+b)
} # 已生成马尔科夫链

B <- burn + 1
x <- X[B:N, ]
cat('Means: ',round(colMeans(x),2))

plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c(expression(x),expression(y)),col=1:2,lwd=2)
```

## Question 4 (Extension)

For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$.

## Answer

### Experiments

```{r,echo=FALSE}
rm(list=ls())
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}
f <- function(x, theta,eta) {
  num <- theta * pi * (1 + ((x - eta)/theta)^2)
  1/num
} # theta > 0
m <- 30000
theta <- 1
eta <- 0
# 提议分布为N(X,sigma^2)
sigma <- 4
norm_chain <- function(m,x0,sigma){
  x <- numeric(m)
  x[1] <- rnorm(n = 1,mean = x0,sd = sigma) 
  k <- 0
  u <- runif(m)
  for (i in 2:m) {
    xt <- x[i-1]
    y <- rnorm(n = 1,mean = xt,sd = sigma)
    num <- f(y, theta, eta) * dnorm(xt,mean = y,sd = sigma)
    den <- f(xt, theta, eta) * dnorm(y,mean = xt,sd = sigma)
    if (u[i] <= num/den){
      x[i] <- y
    } else {
      x[i] <- xt
      k <- k+1     #y is rejected
    }
  }
  return(x)
}
k <- 4        #number of chains to generate
x0 <- c(-20,-10,10,20)
#generate the chains
#set.seed(12345)
X <- matrix(0, nrow=k, ncol=m)
for (i in 1:k)
  X[i, ] <- norm_chain(m,x0[i],sigma)

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))

b <- 1000
for (i in 1:k){
  if(i==1){
    plot((b+1):m,psi[i, (b+1):m], type="l",
            xlab='Index', ylab=bquote(phi),ylim = c(-1,1))
  }else{
    lines(psi[i, (b+1):m], col=i)
  }
}
rhat <- rep(0, m)
for (j in (b+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

```{r,echo=FALSE}
rm(list=ls())
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}
m <- 30000
a <- 1
b <- 1
n <- 10
biviarate_chain <- function(N,x0){
  X <- matrix(0, N, 2) #the chain, a bivariate sample
  X[1, ] <- x0 #initialize
  for (i in 2:N) {
    x2 <- X[i-1, 2]
    X[i, 1] <- rbinom(1,n,x2)
    x1 <- X[i, 1]
    X[i, 2] <- rbeta(1,x1+a,n-x1+b)
  } # 已生成马尔科夫链
  X
}
k <- 4        #number of chains to generate
x0 <- matrix(0, nrow=k, ncol=2)
x0[,1] <- runif(4,0,10)
#generate the chains
X <- matrix(0, nrow=k, ncol=m)
M <- matrix(0, nrow=m, ncol=2)
for (i in 1:k){
  M <- biviarate_chain(m,x0[i,])
  X[i, ] <- M[,1]
}

x0 <- matrix(0, nrow=k, ncol=2)
x0[,1] <- rep(n/2,4)
x0[,2] <- runif(4,0,1)
#generate the chains
Y <- matrix(0, nrow=k, ncol=m)
M <- matrix(0, nrow=m, ncol=2)
for (i in 1:k){
  M <- biviarate_chain(m,x0[i,])
  Y[i, ] <- M[,2]
}

#compute diagnostic statistics
psi1 <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi1))
  psi1[i,] <- psi1[i,] / (1:ncol(psi1))

b <- 1000
for (i in 1:k){
  if(i==1){
    plot((b+1):m,psi1[i, (b+1):m], type="l",
         xlab='Index', ylab=bquote(phi),ylim = c(4,6))
  }else{
    lines(psi1[i, (b+1):m], col=i)
  }
}
rhat <- rep(0, m)
for (j in (b+1):m)
  rhat[j] <- Gelman.Rubin(psi1[,1:j])
plot(rhat[(b+1):m], type="l", xlab="", ylab="R"
     ,main = "Y初始值不变，X初始值变化")
abline(h=1.2, lty=2)

#compute diagnostic statistics
psi2 <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi2))
  psi2[i,] <- psi2[i,] / (1:ncol(psi2))

for (i in 1:k){
  if(i==1){
    plot((b+1):m,psi2[i, (b+1):m], type="l",
         xlab='Index', ylab=bquote(phi),ylim = c(0.4,0.6))
  }else{
    lines(psi2[i, (b+1):m], col=i)
  }
}
rhat <- rep(0, m)
for (j in (b+1):m)
  rhat[j] <- Gelman.Rubin(psi2[,1:j])
plot(rhat[(b+1):m], type="l", xlab="", ylab="R"
     ,main = "X初始值不变，Y初始值变化")
abline(h=1.2, lty=2)
```

# Homework-2024-11-04

## Question 1 (Exercise 11.3 in "Statistical Computing with R")

(a). Write a function to compute the $k^{\text {th }}$ term in

$$
\sum_{k=0}^{\infty} \frac{(-1)^k}{k!2^k} \frac{\|a\|^{2 k+2}}{(2 k+1)(2 k+2)} \frac{\Gamma\left(\frac{d+1}{2}\right) \Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)}
$$

where $d \geq 1$ is an integer, $a$ is a vector in $\mathbb{R}^d$, and $\|\cdot\|$ denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large $k$ and $d$. (This sum converges for all $a \in \mathbb{R}^d$ ).

(b). Modify the function so that it computes and returns the sum.

(c). Evaluate the sum when $a=(1,2)^T$.

## Answer

### Analysis

Factorial operations, power function operations, Euclidean norm operations, and gamma function operations have numerical calculation problems when the dimension $d$ and the term $k$ are too large, and logarithmic operations can be used to nest into exponential operations to process too large or too small data.

### Experiments

```{r}
rm(list=ls())
# 计算出项数为k_term，维数为dim，向量为vector_a的值
numerical <- function(k_term,dim,vector_a){
  factorial_k <- numeric(k_term)
  for (i in 1:k_term) factorial_k[i] <- log(i)
  # 先对数化
  term_1 <- -(k_term*log(2)+sum(factorial_k))
  norm_a <- sum(vector_a^2)
  term_2 <- (k_term+1)*log(norm_a)
  term_3 <- lgamma((dim+1)/2)+lgamma(k_term+1.5)-lgamma(k_term+dim/2+1)
  term <- term_1+term_2+term_3
  # 再指数化代入
  result <- (-1)^k_term * exp(term) / ((2*k_term+1)*(2*k_term+2))
  return(result)
}
dim <- 100; vector_a <- rep(2,dim); k_term <- 100
result <- numerical(k_term,dim,vector_a)
round(result,4)
# 项数求和，最终定会收敛
sum_numerical <- function(d,a){
  k <- 1
  minus <- numerical(k,d,a)
  Sum <- 0
  while (abs(minus) >= 1e-8) {
    Sum <- Sum + minus; k <- k+1 
    minus <- numerical(k,d,a)
  }
  Sum
}
d <- 2 ; a <- c(1,2); SUM <- sum_numerical(d,a)
round(SUM,4)

```

## Question 2 (Exercise 11.5 in "Statistical Computing with R")

Write a function to solve the equation

$$
\begin{aligned}
& \frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_0^{c_{k-1}}\left(1+\frac{u^2}{k-1}\right)^{-k / 2} d u \\
& \quad=\frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_0^{c_k}\left(1+\frac{u^2}{k}\right)^{-(k+1) / 2} d u
\end{aligned}
$$

for $a$, where

$$
c_k=\sqrt{\frac{a^2 k}{k+1-a^2}} .
$$


Compare the solutions with the points $A(k)$ in Exercise 11.4.

## Answer

### Analysis

We use the dichotomy to solve the root of the equations between 11.4 and 11.5. In computer science, a binary search algorithm, also known as a halved search algorithm, is a search algorithm that finds a specific element in an ordered array. The search process starts with the middle element of the array, and ends if the middle element is exactly the one you want to find; If a particular element is larger or smaller than the intermediate element, it is found in the half of the array that is greater or smaller than the intermediate element, and the comparison starts with the intermediate element as at the beginning. If the array is empty at a step, it cannot be found. This search algorithm narrows the search in half with each comparison.

### Experiments

The left side of the two equations is positive near 0 and negative near 1. The specific code is as follows: 
```{r}
rm(list = ls())
f1 <- function(u, k) {
  below <- 1 + u ^ 2 / (k - 1)
  re <- -(k / 2) * exp(below)
  re
}
f2 <- function(u, k) {
  below <- 1 + u ^ 2 / k
  re <- -((k + 1) / 2) * exp(below)
  re
}
c <- function(a, k)
  sqrt(a ^ 2 * k / (k + 1 - a ^ 2))

rootfinding <- function(a, k) {
  coef <- sqrt(k / (k - 1)) * exp(2 * lgamma(k / 2) - lgamma((k - 1) / 2) -
                                    lgamma((k + 1) / 2))
  res1 <- integrate(f1,
                    lower = 0,
                    upper = c(a, k - 1),
                    k = k)
  res2 <- integrate(f2,
                    lower = 0,
                    upper = c(a, k),
                    k = k)
  re <- coef * res1$value - res2$value
  re
}
# 11.5结果：
K <- 25
res1 <- function(k) {
  i = 1
  # yleft <- 1; yright <- -1;
  xleft <- 0
  xright <- sqrt(k)
  mid <- rootfinding((xleft + xright) / 2, k)
  epsilon1 <- 1e-6
  de <- mid
  while (abs(de) >= epsilon1) {
    if (mid > 0) {
      xleft <- (xleft + xright) / 2
      de <- mid - rootfinding((xleft + xright) / 2, k)
      mid <- rootfinding((xleft + xright) / 2, k)
    }
    else {
      xright <- (xleft + xright) / 2
      de <- mid - rootfinding((xleft + xright) / 2, k)
      mid <- rootfinding((xleft + xright) / 2, k)
    }
    i <- i + 1
  }
  return((xleft + xright) / 2)
}
# 11.5的A(k):
root <- res1(K)
round(root, 4)

curves_intersection <- function(a, k) {
  S_lower <- 1 - pt(q = c(a, k - 1), df = k - 1)
  S_upper <- 1 - pt(q = c(a, k), df = k)
  return(S_lower - S_upper)
}
res2 <- function(k) {
  i = 1
  # yleft <- 1; yright <- -1;
  xleft <- 0
  xright <- sqrt(k)
  mid <- curves_intersection((xleft + xright) / 2, k)
  epsilon1 <- 1e-6
  de <- mid
  while (abs(de) >= epsilon1) {
    if (mid > 0) {
      xleft <- (xleft + xright) / 2
      de <- mid - curves_intersection((xleft + xright) / 2, k)
      mid <- curves_intersection((xleft + xright) / 2, k)
    }
    else {
      xright <- (xleft + xright) / 2
      de <- mid - curves_intersection((xleft + xright) / 2, k)
      mid <- curves_intersection((xleft + xright) / 2, k)
    }
    i <- i + 1
  }
  return((xleft + xright) / 2)
}
# 11.4的A(k):
root <- res2(K)
round(root, 4)

```
There is a gap between the roots solved by the two equations, which may be related to the fact that the distribution function values are too close to each other due to the use of the t-distribution.

## Question 3 (Extension)

Suppose $T_1, \ldots, T_n$ are i.i.d. samples drawn from the exponential distribution with expectation $\lambda$. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $Y_i=T_i I\left(T_i \leq \tau\right)+\tau I\left(T_i>\tau\right)$, $i=1, \ldots, n$. Suppose $\tau=1$ and the observed $Y_i$ values are as follows:

$$
0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85
$$


Use the E-M algorithm to estimate $\lambda$, compare your result with the observed data MLE (note: $Y_i$ follows a mixture distribution).

## Answer

### Analysis

加入我们已知某个样本的因变量 $T_i$ 是删失的，在实验中途退出。假如退出时间为$3$，即 $T_i \geq 3$。或者我们可以记为 $T_i=3+$ （后面我们将会谈到，这种删失我们称之为右删失）。在原本的似然中我们应该乘入的项本是 $f_\theta\left(T_i\right)$ ，但由于我们不知道样本的具体取值，这时候似然中乘入的项就应该变成 $P_\theta\left(T_i \geq 3\right)$ 。由于似然是一个相对概念，只要我们对每一个参数 $\theta$ 都用上述的代入方式，似然仍能够表达出相对可能的结果。注意到 $S(t)=P(T \geq t)$ ，之后我们都会用这样的表达。

正式写一下现在的似然，假设有一组样本 $\left\{t_i, \delta_i, x_i\right\}_{i=1}^n$ 。其中:$t_i$ 代表了跟进时间（following-up time），是生存时间 $T_i$ 与删失时间 $C_i$ 的较小值。$\delta_i$ 代表了状态 (status)： $\delta_i=1$, if $t_i=T_i$; $\delta_i=0$, if $t_i=C_i$. $x_i$ 代表了协变量（covariates），包括感兴趣的其他变量（如身高血压肺活量......）

则其似然（右删失情况）可以表示成：

$$
L\left(\theta ;\left\{t_i\right\}_{i=1}^n\right)=\prod_i\left(f\left(t_i ; \theta\right)\right)^{\delta_i}\left(S\left(t_i ; \theta\right)\right)^{1-\delta_i}
$$


我们来直观理解一下这个式子: 当 $\delta_i=1$ ，代表样本没有右删失，我们直接可以乘入 $f_\theta(t)$ 。而若 $\delta_i=0$ ，则代表样本右删失，我们需要乘相应的生存函数。由于每一个 $\theta$ 我们都是应用同一套法则，所以该似然还能表示相对可能。

再补充一些EM算法的细节：

(1). E-step: 结合前文的似然函数公式，其具体的$Q(\lambda,\lambda_0)$为：$$Q(\lambda,\lambda_0) = \Sigma_i \mathbb{E}[\delta_i|y_{ob},\lambda_0](-log(\lambda)-\frac{x_i}{\lambda}) + (1 - \mathbb{E}[\delta_i|y_{ob},\lambda_0]) (-\frac{\tau}{\lambda}).$$此式是代入下式的结果：$f(x_i) = \frac{1}{\lambda}\exp(-\frac{x_i}{\lambda}),x_i>0$，$\mathbb{P}\{X_i\geq\tau\} = \exp(-\frac{\tau}{\lambda})$。代入$\mathbb{E}[\delta_i|y_{ob},\lambda_0]$的具体表达式：$$\mathbb{E}[\delta_i|y_{ob},\lambda_0] = \frac{p(X=7,\delta_i=1|\lambda_0)p(\delta_i=1|\lambda_0)}{p(X=7|\lambda_0)},$$其中：$X|\lambda_0 \sim B(10,p),\ p=\mathbb{P}\{X_i\leq\tau|\lambda_0\}=1-\exp(-\frac{\tau}{\lambda_0}) .$ 综上：$$\mathbb{E}[\delta_i|y_{ob},\lambda_0] = \frac{\binom{9}{6}(1-\exp(-\frac{\tau}{\lambda_0}))^6(\exp(-\frac{\tau}{\lambda_0}))^3(1-\exp(-\frac{\tau}{\lambda_0}))}{\binom{10}{7}(1-\exp(-\frac{\tau}{\lambda_0}))^7(\exp(-\frac{\tau}{\lambda_0}))^3}.$$

(2). M-step: 对E-step算出的$Q$函数作最优化即可，要求的是函数极大值点。

### Experiments

```{r}
rm(list = ls())
y <- c(0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85)
tau <- 1
trim <- rep(1,length(y))
trim[which(y == 1)] <- 0
mlogL <- function(lambda = 1) {
  # minus log-likelihood
  n <- length(trim)
  t <- y
  t[which(trim == 0)] <- 0
  # 统计未被截断的观察值个数
  trimmed <- length(which(trim == 1))
  lambda <- abs(lambda)
  # cons + algebra 为似然函数
  cons <- - trimmed * log(lambda) - (n-trimmed) * (tau/lambda)
  algebra <- -sum(t)/lambda
  return(-(cons + algebra))
}
library(stats4)
fit <- mle(mlogL) # No additional data are allowed!
cat("MLE下的参数lambda估计值:",abs(as.numeric(fit@coef)))

# 初始化：
lambda0 <- 1

# EM算法实现
for (step in 1:200) {
  # E-step：
  op <- function(lambda){
    n <- length(trim)
    t <- y
    t[which(trim == 0)] <- 0
    trimmed <- length(which(trim == 1))
    lambda <- abs(lambda)
    # 计算条件期望
    delta <- choose(n-1,trimmed-1)*(1-exp(-tau/lambda0))^(trimmed-1)*(exp(-tau/lambda0))^(n-trimmed)
    delta <- delta * (1 - exp(-tau/lambda0))
    delta <- delta / (choose(n,trimmed)*(1-exp(-tau/lambda0))^(trimmed)*(exp(-tau/lambda0))^(n-trimmed))
    # fterm+sterm 是似然函数取条件期望的结果
    fterm <- delta * (-log(lambda)-mean(y)/lambda)
    sterm <- -(1-delta) * (tau/lambda)
    return(fterm+sterm)
  }
  oldlambda <- lambda0
  # M-step：最大化
  res <- optimize(op,lower=1e-5,upper=1e5,maximum=TRUE)
  lambda0 <- abs(res$objective)
  # 设阈值：当上一步迭代得到的参数与下一步迭代得到的参数变化很小，即认为收敛
  threshold <- 1e-5
  if (sum(abs(lambda0 - oldlambda)) < threshold) break
}
cat("EM算法下的参数lambda估计值:",abs(lambda0))
```

# Homework-2024-11-11

## Question 1 (Exercise 11.7 in "Statistical Computing with R")

Use the simplex algorithm to solve the following problem.
Minimize $4 x+2 y+9 z$ subject to

$$
\begin{aligned}
& 2 x+y+z \leq 2 \\
& x-y+3 z \leq 3 \\
& x \geq 0, y \geq 0, z \geq 0
\end{aligned}
$$


## Answer

```{r}
rm(list = ls())
library(Rglpk)
obj <- c(4,2,9); dir <- c("<=","<=",">=",">=",">=")
mat <- matrix(c(2,1,1,0,0,1,-1,0,1,0,1,3,0,0,1),nrow = 5)
rhs <- c(2,3,0,0,0)
max <- FALSE
Rglpk_solve_LP(obj, mat, dir, rhs,  max = max)
```

So the result above is: $x=0,y=0,z=0$.

## Question 2 (Exercise 11.1.2 in "Advanced R")

### 3 

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```
formulas <- list(
    mpg ~ disp,mpg ~ I(1 / disp),mpg ~ disp + wt,
    mpg ~ I(1 / disp) + wt
)
```

### Answer

```{r}
rm(list = ls())
attach(mtcars)
formulas <- list(mpg ~ disp,mpg ~ I(1 / disp),
                 mpg ~ disp + wt,mpg ~ I(1 / disp) + wt)
lapply(formulas, lm)
```

### 4 

Fit the model mpg ~ disp to each of the bootstrap replicates
of mtcars in the list below by using a for loop and lapply().
Can you do it without an anonymous function?
```
bootstraps <- lapply(1:10, function(i) {
    rows <- sample(1:nrow(mtcars), rep = TRUE)
    mtcars[rows, ]
})
```

### Answer

```{r}
rm(list = ls())
attach(mtcars)
bootstraps <- lapply(1:10, function(i) {
    rows <- sample(1:nrow(mtcars), rep = TRUE)
    mtcars[rows, ]
})
lapply(1:10,function(i) lm(bootstraps[[i]]$mpg~bootstraps[[i]]$disp))
library(purrr)
fit_models <- map(bootstraps, ~lm(mpg ~ disp, data = .))
fit_models
```

The result is displayed above, and I can do it without an anonymous function by using "map()" function.

### 5 

For each model in the previous two exercises, extract $R^2$ using the function below.
```
rsq <- function(mod) summary(mod)$r.squared
```
### Answer

```{r}
rm(list = ls())
attach(mtcars)
rsq <- function(mod) summary(mod)$r.squared
# 3
formulas <- list(mpg ~ disp,mpg ~ I(1 / disp),
                 mpg ~ disp + wt,mpg ~ I(1 / disp) + wt)
ex3 <- lapply(formulas, lm)
lapply(ex3, rsq)
# 4
bootstraps <- lapply(1:10, function(i) {
    rows <- sample(1:nrow(mtcars), rep = TRUE)
    mtcars[rows, ]
})
ex4 <- lapply(1:10,function(i) lm(bootstraps[[i]]$mpg~bootstraps[[i]]$disp))
lapply(ex4, rsq)
```

## Question 3 (Exercise 11.2.5 in "Advanced R")

### 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
```
trials <- replicate(
    100,
    t.test(rpois(10, 10), rpois(7, 10)),
    simplify = FALSE
)
```

Extra challenge: get rid of the anonymous function by using [[ directly.

### Answer

```{r}
rm(list = ls())
trials <- replicate(100,t.test(rpois(10, 10), rpois(7, 10)),simplify = FALSE)
sapply(1:100,function(i) trials[[i]]$p.value)
sapply(trials, "[[", "p.value")
```

### 6

Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

### Answer

```{r}
# 创建一个自定义函数 parallel_lapply
parallel_lapply <- function(input_list, fun, output_type, ...) {
  if(output_type == "vector") {
    result <- vapply(input_list, fun, FUN.VALUE = numeric(1), ...)
  } else if(output_type == "matrix") {
    result <- matrix(unlist(Map(fun, input_list)), nrow = length(input_list))
  } else {
    stop("Invalid output type. Please specify 'vector' or 'matrix'.")
  }
  
  return(result)
}

# 示例输入列表
input_list <- list(1:3, 4:6, 7:9)

# 调用 parallel_lapply 函数来计算每个向量的平均值并存储在向量中
output_vector1 <- parallel_lapply(input_list,mean, "vector")
output_vector2 <- parallel_lapply(input_list,mean, "matrix")

# 输出结果
output_vector1
output_vector2

# 比较parallel_lapply与lapply
lapply(input_list,mean)
```

## Question 4 (Exercise 17.5.1 in "Advanced R")

### 4

Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition (http://en.wikipedia.org/wiki/Pearson\%27s_chi-squared_test).

### Answer

```{r}
fast_chisq_test <- function(x, y) {
  obs <- table(x, y)
  n <- sum(obs)
  
  # 计算期望频数
  exp_freq <- outer(rowSums(obs), colSums(obs)) / n
  
  # 计算卡方统计量
  chisq_stat <- sum((obs - exp_freq)^2 / exp_freq)
  
  return(chisq_stat)
}

# 示例数据
x <- c(1, 2, 1, 2, 1)
y <- c(1, 1, 2, 2, 1)

# 调用 fast_chisq_test 函数计算卡方统计量
chisq_stat <- fast_chisq_test(x, y)

# 输出结果
chisq_stat
```

### 5

Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

### Answer

```{r}
fast_table <- function(x, y) {
  max_x <- max(x)
  max_y <- max(y)
  
  counts <- matrix(0, nrow = max_x, ncol = max_y)
  
  for (i in 1:length(x)) {
    counts[x[i], y[i]] <- counts[x[i], y[i]] + 1
  }
  
  return(counts)
}

fast_chisq_test <- function(x, y) {
  obs <- fast_table(x, y)
  n <- sum(obs)
  
  # 计算期望频数
  exp_freq <- outer(rowSums(obs), colSums(obs)) / n
  
  # 计算卡方统计量
  chisq_stat <- sum((obs - exp_freq)^2 / exp_freq)
  
  return(chisq_stat)
}

# 示例数据
x <- c(1, 2, 1, 2, 1)
y <- c(1, 1, 2, 2, 1)

# 调用 fast_chisq_test 函数计算卡方统计量
chisq_stat <- fast_chisq_test(x, y)

# 输出结果
chisq_stat
```

# Homework-2024-11-18

## Question 

### Question 1

Write an Rcpp function for Exercise 9.8 (page 278, Statistical Computing with R).

### Question 2

Compare the corresponding generated random numbers with those by the R function you wrote using the function “qqplot”.

### Question 3

Campare the computation time of the two functions with the function “microbenchmark”.

### Question 4

Comments your results.

## Answer

### Answer 1

在之前的作业中，生成练习9.8的马尔科夫链时，R语言代码如下：

```{r,eval=FALSE}
rm(list=ls())
#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
a <- 1
b <- 1
n <- 10
###### generate the chain #####

X[1, ] <- c(n/2, 0.5) #initialize
for (i in 2:N) {
  x2 <- X[i-1, 2]
  X[i, 1] <- rbinom(1,n,x2)
  x1 <- X[i, 1]
  X[i, 2] <- rbeta(1,x1+a,n-x1+b)
} # 已生成马尔科夫链

B <- burn + 1
x <- X[B:N, ]
cat('Means: ',round(colMeans(x),2))

plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c(expression(x),expression(y)),col=1:2,lwd=2)
```

现通过简化，用一个函数实现Gibbs采样：

```{r,eval=FALSE}
gibbsSamplingR <- function(N, thin) {
  mat <- matrix(nrow = N, ncol = 2)
  n = 10; a = 1; b = 1; x = n/2; y = 0.5
  for (i in 1:N) {
    for (j in 1:thin) {
      x = rbinom(1, n, y)
      y = rbeta(1, x + a,n - x + b)
    }
    mat[i, ] <- c(x, y)
  }
  mat
}
```

沿相同思路，其C++代码如下：

```
#include <Rcpp.h>
using namespace Rcpp;
// [[Rcpp::export]]
NumericMatrix gibbsSamplingC(int N, int thin) {
  //N = 500; thin = 10
  NumericMatrix mat(N, 2);
  int n = 10;
  double a = 1, b = 1;
  int x = n/2;
  double y = 0.5;
  for(int i = 0; i < N; i++) {
    for(int j = 0; j < thin; j++) {
      x = rbinom(1, n, y)[0];
      y = rbeta(1, x + a,n - x + b)[0];
    }
    mat(i, 0) = x;
    mat(i, 1) = y;
  }
  return(mat);
}
```

### Answer 2

```{r}
rm(list=ls())
library(Rcpp)
library(SA24204187)
N = 500; thin <- 10
GibbsR <- gibbsSamplingR(N,thin)
GibbsC <- gibbsSamplingC(N,thin)
par(mfrow=c(1,2))
plot(GibbsR[,1],type='l',col=1,lwd=2,xlab='Index-GibbsR',ylab='Random numbers')
lines(GibbsR[,2],col=2,lwd=2)
plot(GibbsC[,1],type='l',col=1,lwd=2,xlab='Index-GibbsR',ylab='Random numbers')
lines(GibbsC[,2],col=2,lwd=2)
layout(1)
```

数据确是随机生成的，整个马尔科夫链细节上不同，却在大体上都可看出，似乎两组数据中x均值以及y的均值相同。

### Answer 3

```{r}
rm(list=ls())
library(Rcpp)
library(SA24204187)
N = 500; thin <- 10
GibbsR <- gibbsSamplingR(N,thin)
GibbsC <- gibbsSamplingC(N,thin)
library(microbenchmark)
ts <- microbenchmark(GibbsR = gibbsSamplingR(N,thin),GibbsC = gibbsSamplingC(N,thin))
summary(ts)[,c(1,3,5,6)]
```

显而易见，使用相同方法进行Gibbs采样，用Rcpp运行速度是直接用R
的大约九倍。

### Answer 4

涉及到多循环、向量化处理数据的编程内容时，用Rcpp运行速度往往快过直接使用R语言运行。

